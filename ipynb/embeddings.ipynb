{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nBdagXKxEsR",
        "outputId": "2a357d42-c930-4c5f-fb13-f3e2da52d285"
      },
      "outputs": [],
      "source": [
        "#Install Dependencies\n",
        "\n",
        "!pip install torch torchvision pinecone pillow cloudinary\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import json\n",
        "import cloudinary\n",
        "import cloudinary.uploader\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "\n",
        "PROJECT_NAME = 'visual-search'\n",
        "COMMON_DIMENSION = 2048\n",
        "DATASET_PATH = '/content/drive/MyDrive/datasets/afhq/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm_cpY1F6dR4",
        "outputId": "78cdc944-3539-4aeb-8314-208b78629f2b"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY-tsVxz7sYQ"
      },
      "outputs": [],
      "source": [
        "# Dataset class for AFHQ\n",
        "class AFHQDataset(Dataset):\n",
        "  def __init__(self, root_dir, limit=500, transform = None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.label_map = {'cat': 0, 'dog': 1, 'wild': 2}\n",
        "\n",
        "        #Load image paths\n",
        "        for category in ['wild']: #['cat', 'dog', 'wild']\n",
        "          category_dir = os.path.join(root_dir, category)\n",
        "          if os.path.exists(category_dir):\n",
        "            for image_name in os.listdir(category_dir):\n",
        "              if image_name.endswith(('.jpg', '.png', '.jpeg')) and limit > 0:\n",
        "                self.images.append(os.path.join(category_dir, image_name))\n",
        "                self.labels.append(category)\n",
        "                limit -= 1\n",
        "          else:\n",
        "            print(f\"Category {category} not found\")\n",
        "\n",
        "        print(f\"Loaded {len(self.images)} images\")\n",
        "        print(f\"Categories: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.images[index]\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    label = self.labels[index]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label, image_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJlGw5G59nco"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction Model\n",
        "\n",
        "class FeatureExtractor:\n",
        "  def __init__(self, model_name='resetnet50', use_clip=False):\n",
        "    if use_clip:\n",
        "      # Use clip for better semantic understanding\n",
        "      !pip install transformers\n",
        "      from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "      self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "      self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "      self.model.to(device)\n",
        "      self.model.eval()\n",
        "      self.dim = 512  # CLIP dimension\n",
        "\n",
        "    else:\n",
        "      # Use ResNet50 for visual features\n",
        "      model = models.resnet50(pretrained=True)\n",
        "\n",
        "      # Remove the final classification layer\n",
        "      self.model = nn.Sequential(*list(model.children())[:-1])\n",
        "      self.model.to(device)\n",
        "      self.model.eval()\n",
        "      self.dim = 2048  # ResNet50 dimension\n",
        "\n",
        "    self.use_clip = use_clip\n",
        "\n",
        "    # Describe preprocessing for images\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "    ])\n",
        "\n",
        "  def extract(self, image_path):\n",
        "      if self.use_clip:\n",
        "          image = Image.open(image_path).convert('RGB')\n",
        "          inputs = self.processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              features = self.model.get_image_features(**inputs)\n",
        "              features = features.cpu().numpy().flatten()\n",
        "      else:\n",
        "          image = Image.open(image_path).convert('RGB')\n",
        "          image_tensor = self.transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              features = self.model(image_tensor)\n",
        "              features = features.cpu().numpy().flatten()\n",
        "\n",
        "      # L2 normalize for better similarity search\n",
        "      features = features / np.linalg.norm(features)\n",
        "      return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzM6y1QvzRRl",
        "outputId": "aa802744-5b0a-4bdd-94eb-88fe843fc3c2"
      },
      "outputs": [],
      "source": [
        "# Extract Feature from AFHQ Dataset\n",
        "\n",
        "# Setup Cloudinary\n",
        "cloudinary.config(\n",
        "    cloud_name = userdata.get('CLOUDINARY_CLOUD_NAME'),\n",
        "    api_key = userdata.get('CLOUDINARY_API_KEY'),\n",
        "    api_secret = userdata.get('CLOUDINARY_API_SECRET'),\n",
        "    secure=True\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize feature extractor\n",
        "extractor = FeatureExtractor(model_name='resnet50', use_clip=False)\n",
        "print(f\"Feature dimension: {extractor.dim}\")\n",
        "\n",
        "\n",
        "dataset = AFHQDataset(DATASET_PATH, limit=3200, transform=extractor.transform)\n",
        "\n",
        "# extract features for all image categories\n",
        "features_data = []\n",
        "batch_size = 32\n",
        "\n",
        "print(\"Extracting features...\")\n",
        "for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "  batch_end = min(i + batch_size, len(dataset))\n",
        "\n",
        "  for j in range(i, batch_end):\n",
        "    image, label, image_path = dataset[j]\n",
        "\n",
        "    try:\n",
        "\n",
        "      features = extractor.extract(image_path)\n",
        "\n",
        "      # Upload to Cloudinary\n",
        "      cloudinary_result = cloudinary.uploader.upload(\n",
        "          image_path,\n",
        "          folder=f\"afhq/{label}\",\n",
        "          use_filename=True,\n",
        "          unique_filename=False,\n",
        "          overwrite=True\n",
        "      )\n",
        "\n",
        "      # Create feature dictionary\n",
        "      feature_dict = {\n",
        "          'id': f\"{label}_{os.path.basename(image_path)}\",\n",
        "          'values': features.tolist(),\n",
        "          'metadata': {\n",
        "              'project': PROJECT_NAME,\n",
        "              'category': label,\n",
        "              'filename': os.path.basename(image_path),\n",
        "              'path': image_path,\n",
        "              'animal_type': label,\n",
        "              'image_url': cloudinary_result.get(\"secure_url\")\n",
        "          }\n",
        "      }\n",
        "\n",
        "      features_data.append(feature_dict)\n",
        "\n",
        "    except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features_data)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zt2QFfRCYlm",
        "outputId": "26c9abd1-cb9b-4dcd-f315-b72c2fa8164e"
      },
      "outputs": [],
      "source": [
        "# Setup Pinecone\n",
        "\n",
        "pc = Pinecone(\n",
        "        api_key=userdata.get('PINECONE_API_KEY')\n",
        "    )\n",
        "\n",
        "\n",
        "# Create index\n",
        "INDEX_NAME = \"multi-project-index\"\n",
        "\n",
        "# Create/Use existing index if it exists\n",
        "if not pc.has_index(INDEX_NAME):\n",
        "  print(f\"Creating new index: {INDEX_NAME}\")\n",
        "  pc.create_index(\n",
        "      name=INDEX_NAME,\n",
        "      dimension=COMMON_DIMENSION,\n",
        "      metric=\"cosine\",\n",
        "      spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n",
        "else:\n",
        "    print(f\"Using existing index: {INDEX_NAME}\")\n",
        "\n",
        "# Wait for index to be ready\n",
        "import time\n",
        "while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# Check current stats\n",
        "stats = index.describe_index_stats()\n",
        "print(f\"\\nCurrent index stats: {stats}\")\n",
        "print(f\"Total vectors: {stats.total_vector_count}\")\n",
        "if stats.namespaces:\n",
        "    print(f\"Namespaces: {list(stats.namespaces.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o6CKrIXMGg_",
        "outputId": "9a0b5a1b-869f-4240-c7a7-0ee2aa5be656"
      },
      "outputs": [],
      "source": [
        "# Upload to current namespace\n",
        "namespace = PROJECT_NAME\n",
        "batch_size = 100\n",
        "\n",
        "print(f\"\\nUploading to namespace: {namespace}\")\n",
        "\n",
        "# Check how much space we have left\n",
        "current_vectors = stats.total_vector_count\n",
        "free_tier_limit = 100000\n",
        "space_left = free_tier_limit - current_vectors\n",
        "print(f\"Space used: {current_vectors}/{free_tier_limit} ({current_vectors/free_tier_limit*100:.1f}%)\")\n",
        "print(f\"Space for new vectors: {space_left}\")\n",
        "\n",
        "if len(features_data) > space_left:\n",
        "    print(f\"WARNING: Not enough space! Reducing to {space_left} vectors\")\n",
        "    features_data = features_data[:space_left]\n",
        "\n",
        "# Upload in batches\n",
        "for i in tqdm(range(0, len(features_data), batch_size)):\n",
        "    batch = features_data[i:i + batch_size]\n",
        "\n",
        "    vectors = [\n",
        "        {\n",
        "            \"id\": item['id'],\n",
        "            \"values\": item['values'],\n",
        "            \"metadata\": item['metadata']\n",
        "        }\n",
        "        for item in batch\n",
        "    ]\n",
        "\n",
        "    # Upsert to namespace\n",
        "    index.upsert(vectors=vectors, namespace=namespace)\n",
        "\n",
        "stats = index.describe_index_stats()\n",
        "print(f\"\\nUpdated index stats:\")\n",
        "print(f\"Total vectors: {stats.total_vector_count}\")\n",
        "if namespace in stats.namespaces:\n",
        "    print(f\"Vectors in {namespace}: {stats.namespaces[namespace]['vector_count']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
